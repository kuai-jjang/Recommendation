{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "class close_words:\n",
    "    def __init__(self,model,w2i,pca_1=False,n=5):\n",
    "        self.model=model\n",
    "        self.w2i=w2i\n",
    "        self.n=n\n",
    "        self.i2w=dict(zip(w2i.values(),w2i.keys()))\n",
    "        pca = PCA(n_components=10)\n",
    "        self.embeddings=model['embedding_in.weight'].detach() if not pca_1 else torch.tensor(pca.fit_transform(model['embedding_in.weight'].detach()))\n",
    " \n",
    "    def input_word(self,word):\n",
    "        self.sample_idx=self.w2i[word]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "    \n",
    "    def sample_word(self):\n",
    "        \n",
    "        self.sample_idx=random.sample(list(self.w2i.values()),1)[0]\n",
    "        self.sampled_word=self.i2w[self.sample_idx]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "        print('임의의 단어:',self.sampled_word)\n",
    "    \n",
    "    def l2_dist(self):\n",
    "        \n",
    "        trial=torch.mul((self.embeddings-self.sample_vec),(self.embeddings-self.sample_vec))\n",
    "        trial[self.sample_idx]=100    \n",
    "        trial=torch.sum(trial,dim=1)\n",
    "        #rec=torch.argmax(-self.trial).item()\n",
    "        idexes=torch.argsort(-trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        print('사전사이즈:',len(self.i2w))\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i): #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "                \n",
    "    def cos_sim(self):\n",
    "        \n",
    "   \n",
    "        upper=torch.sum(torch.mul(self.embeddings,self.sample_vec),dim=1)\n",
    "  \n",
    "        lower=torch.sum(torch.mul(self.embeddings,self.embeddings),dim=1)+torch.sum(torch.mul(self.sample_vec,self.sample_vec))\n",
    "    \n",
    "        trial=torch.div(upper,lower)\n",
    "\n",
    "        trial[self.sample_idx]=-100\n",
    "        idexes=torch.argsort(trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i)+1: #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 사이즈: torch.Size([30655, 256])\n",
      "사전 사이즈: 30654\n"
     ]
    }
   ],
   "source": [
    "#model=torch.load('./w2v_withoud_ns',map_location='cpu')\n",
    "model=torch.load('./training_with_mecab_2018_2_epoch_14',map_location='cpu')['state_dict']\n",
    "\n",
    "with open('./preprocessing//vocab_mecab.pickle','rb') as f:\n",
    "    w2i=pickle.load(f)\n",
    "    \n",
    "print('임베딩 사이즈:',model['embedding_in.weight'].shape)  #unk 랑 embedding이 0부터인것을 몰랐음. 0은 버리면 됨\n",
    "print('사전 사이즈:',len(w2i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 단어: ('제목', 'NNG')\n",
      "사전사이즈: 30654\n",
      "1 번째 가까운 단어: ('자본', 'NNG')\n",
      "2 번째 가까운 단어: ('문사', 'NNG')\n",
      "3 번째 가까운 단어: ('맛보기', 'NNG')\n",
      "4 번째 가까운 단어: ('철두철미', 'MAG')\n",
      "5 번째 가까운 단어: ('니까요', 'VCP+EC')\n",
      "임의의 단어 ('제목', 'NNG')\n",
      "1 번째 가까운 단어: ('맛보기', 'NNG')\n",
      "2 번째 가까운 단어: ('문사', 'NNG')\n",
      "3 번째 가까운 단어: ('자본', 'NNG')\n",
      "4 번째 가까운 단어: ('영양소', 'NNG')\n",
      "5 번째 가까운 단어: ('익힌', 'VV+ETM')\n"
     ]
    }
   ],
   "source": [
    "showing=close_words(model,w2i,pca_1=True,n=5)\n",
    "showing.sample_word()\n",
    "showing.l2_dist()\n",
    "print('임의의 단어',showing.sampled_word)\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전사이즈: 30654\n",
      "1 번째 가까운 단어: ('있', 'VA')\n",
      "2 번째 가까운 단어: ('없', 'VA')\n",
      "3 번째 가까운 단어: ('강의', 'NNG')\n",
      "4 번째 가까운 단어: ('문제', 'NNG')\n",
      "5 번째 가까운 단어: ('같', 'VA')\n",
      "====\n",
      "1 번째 가까운 단어: ('있', 'VA')\n",
      "2 번째 가까운 단어: ('없', 'VA')\n",
      "3 번째 가까운 단어: ('강의', 'NNG')\n",
      "4 번째 가까운 단어: ('않', 'VX')\n",
      "5 번째 가까운 단어: ('같', 'VA')\n"
     ]
    }
   ],
   "source": [
    "showing.input_word(('과제', 'NNG'))\n",
    "showing.l2_dist()\n",
    "print('====')\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNK 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014246669941204219\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessing/vocab_without_josa_gut_su_freq.pickle','rb') as f:\n",
    "    total_freq=pickle.load(f)\n",
    "    \n",
    "with open('./preprocessing/vocab_without_josa_gut_su.pickle','rb') as h:\n",
    "    voc=pickle.load(h)\n",
    "    \n",
    "not_unk=list(voc.keys())\n",
    "\n",
    "freq_sum=0\n",
    "\n",
    "def summing(x):\n",
    "    \n",
    "    global freq_sum\n",
    "    freq_sum+=x\n",
    "\n",
    "_=list(map(lambda x:summing(total_freq[x]),not_unk))\n",
    "\n",
    "ratio=(sum(total_freq.values())-freq_sum)/sum(total_freq.values())\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unk 빈도수가 문제는 아닌듯??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
