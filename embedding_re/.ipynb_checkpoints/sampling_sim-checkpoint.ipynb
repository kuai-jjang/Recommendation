{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "class close_words:\n",
    "    def __init__(self,model,w2i,pca_1=False,n=5):\n",
    "        self.model=model\n",
    "        self.w2i=w2i\n",
    "        self.n=n\n",
    "        self.i2w=dict(zip(w2i.values(),w2i.keys()))\n",
    "        pca = PCA(n_components=10)\n",
    "        self.embeddings=model if not pca_1 else torch.tensor(pca.fit_transform(model))\n",
    " \n",
    "    def input_word(self,word):\n",
    "        self.sample_idx=self.w2i[word]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "    \n",
    "    def sample_word(self):\n",
    "        \n",
    "        self.sample_idx=random.sample(list(self.w2i.values()),1)[0]\n",
    "        self.sampled_word=self.i2w[self.sample_idx]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "        print('임의의 단어:',self.sampled_word)\n",
    "    \n",
    "    def l2_dist(self):\n",
    "        \n",
    "        trial=torch.mul((self.embeddings-self.sample_vec),(self.embeddings-self.sample_vec))\n",
    "        trial[self.sample_idx]=100    \n",
    "        trial=torch.sum(trial,dim=1)\n",
    "        #rec=torch.argmax(-self.trial).item()\n",
    "        idexes=torch.argsort(-trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        print('사전사이즈:',len(self.i2w))\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i): #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "                \n",
    "    def cos_sim(self):\n",
    "        \n",
    "   \n",
    "        upper=torch.sum(torch.mul(self.embeddings,self.sample_vec),dim=1)\n",
    "  \n",
    "        lower=torch.sum(torch.mul(self.embeddings,self.embeddings),dim=1)+torch.sum(torch.mul(self.sample_vec,self.sample_vec))\n",
    "    \n",
    "        trial=torch.div(upper,lower)\n",
    "\n",
    "        trial[self.sample_idx]=-100\n",
    "        idexes=torch.argsort(trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i)+1: #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './training_with_mecab_2018_2_epoch_14'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d87d5d93aa67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model=torch.load('./w2v_withoud_ns',map_location='cpu')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./training_with_mecab_2018_2_epoch_14'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedding_in.weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./preprocessing//vocab_mecab.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mw2i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python3.6\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './training_with_mecab_2018_2_epoch_14'"
     ]
    }
   ],
   "source": [
    "#model=torch.load('./w2v_withoud_ns',map_location='cpu')\n",
    "model=torch.load('./training_with_mecab_2018_2_epoch_14',map_location='cpu')['state_dict']['embedding_in.weight'].detach()\n",
    "\n",
    "with open('./preprocessing//vocab_mecab.pickle','rb') as f:\n",
    "    w2i=pickle.load(f)\n",
    "    \n",
    "print('임베딩 사이즈:',model.shape)  #unk 랑 embedding이 0부터인것을 몰랐음. 0은 버리면 됨\n",
    "print('사전 사이즈:',len(w2i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 단어: ('제목', 'NNG')\n",
      "사전사이즈: 30654\n",
      "1 번째 가까운 단어: ('자본', 'NNG')\n",
      "2 번째 가까운 단어: ('문사', 'NNG')\n",
      "3 번째 가까운 단어: ('맛보기', 'NNG')\n",
      "4 번째 가까운 단어: ('철두철미', 'MAG')\n",
      "5 번째 가까운 단어: ('니까요', 'VCP+EC')\n",
      "임의의 단어 ('제목', 'NNG')\n",
      "1 번째 가까운 단어: ('맛보기', 'NNG')\n",
      "2 번째 가까운 단어: ('문사', 'NNG')\n",
      "3 번째 가까운 단어: ('자본', 'NNG')\n",
      "4 번째 가까운 단어: ('영양소', 'NNG')\n",
      "5 번째 가까운 단어: ('익힌', 'VV+ETM')\n"
     ]
    }
   ],
   "source": [
    "showing=close_words(model,w2i,pca_1=True,n=5)\n",
    "showing.sample_word()\n",
    "showing.l2_dist()\n",
    "print('임의의 단어',showing.sampled_word)\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전사이즈: 30654\n",
      "1 번째 가까운 단어: ('있', 'VA')\n",
      "2 번째 가까운 단어: ('없', 'VA')\n",
      "3 번째 가까운 단어: ('강의', 'NNG')\n",
      "4 번째 가까운 단어: ('문제', 'NNG')\n",
      "5 번째 가까운 단어: ('같', 'VA')\n",
      "====\n",
      "1 번째 가까운 단어: ('있', 'VA')\n",
      "2 번째 가까운 단어: ('없', 'VA')\n",
      "3 번째 가까운 단어: ('강의', 'NNG')\n",
      "4 번째 가까운 단어: ('않', 'VX')\n",
      "5 번째 가까운 단어: ('같', 'VA')\n"
     ]
    }
   ],
   "source": [
    "showing.input_word(('과제', 'NNG'))\n",
    "showing.l2_dist()\n",
    "print('====')\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 사이즈: torch.Size([3399, 256])\n",
      "사전 사이즈: 3399\n"
     ]
    }
   ],
   "source": [
    "#model=torch.load('./w2v_withoud_ns',map_location='cpu')\n",
    "model=torch.load('./doc2vec_2017_1_epoch_4',map_location='cpu')['state_dict']['lecture.weight'].detach()\n",
    "\n",
    "with open('./preprocessing//doc_id_2017_1.pickle','rb') as f:\n",
    "    w2i=pickle.load(f)\n",
    "    \n",
    "print('임베딩 사이즈:',model.shape)  #unk 랑 embedding이 0부터인것을 몰랐음. 0은 버리면 됨\n",
    "print('사전 사이즈:',len(w2i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 단어: ('김영민 교수님', '디자인브랜딩')\n",
      "사전사이즈: 3399\n",
      "1 번째 가까운 단어: ('유상동 교수님', '유전학Ⅰ(영강)')\n",
      "2 번째 가까운 단어: ('오인영 교수님', '유럽지성사')\n",
      "3 번째 가까운 단어: ('권보현 교수님', '미적분학및연습I 의과대학 의예과 17학번')\n",
      "4 번째 가까운 단어: ('권혁명 교수님', '사고와표현Ⅰ 보건정책관리학부1, 28-54')\n",
      "5 번째 가까운 단어: ('이광렬 교수님', '무기화학실험')\n",
      "임의의 단어 ('김영민 교수님', '디자인브랜딩')\n",
      "1 번째 가까운 단어: ('오인영 교수님', '유럽지성사')\n",
      "2 번째 가까운 단어: ('유상동 교수님', '유전학Ⅰ(영강)')\n",
      "3 번째 가까운 단어: ('권보현 교수님', '미적분학및연습I 의과대학 의예과 17학번')\n",
      "4 번째 가까운 단어: ('전휘원 교수님', '독일예술의이해')\n",
      "5 번째 가까운 단어: ('이광렬 교수님', '무기화학실험')\n"
     ]
    }
   ],
   "source": [
    "showing=close_words(model,w2i,pca_1=True,n=5)\n",
    "showing.sample_word()\n",
    "showing.l2_dist()\n",
    "print('임의의 단어',showing.sampled_word)\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전사이즈: 3399\n",
      "1 번째 가까운 단어: ('윤종선 교수님', '사고와표현Ⅰ 물리1, 1-24')\n",
      "2 번째 가까운 단어: ('서형주 교수님', '효소공학')\n",
      "3 번째 가까운 단어: ('Fang-Chi Lu 교수님', '광고론(영강)')\n",
      "4 번째 가까운 단어: ('이경호 교수님', '1학년 세미나 컴퓨터학과, 금4 아산이학관633호')\n",
      "5 번째 가까운 단어: ('홍성태 교수님', '한국사회의구조와변동')\n",
      "====\n",
      "1 번째 가까운 단어: ('윤종선 교수님', '사고와표현Ⅰ 물리1, 1-24')\n",
      "2 번째 가까운 단어: ('서형주 교수님', '효소공학')\n",
      "3 번째 가까운 단어: ('이수은 교수님', '한국음악의세계')\n",
      "4 번째 가까운 단어: ('홍성태 교수님', '한국사회의구조와변동')\n",
      "5 번째 가까운 단어: ('현광호 교수님', '한국사개론 <한국사의재조명> 유사과목, 핵심교양인정 신청 가능(포털공지>학사일정 참조)')\n"
     ]
    }
   ],
   "source": [
    "showing.input_word(('Simon Kim 교수님', '프랑스어말하기(외국어강의)'))\n",
    "showing.l2_dist()\n",
    "print('====')\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNK 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014246669941204219\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessing/vocab_without_josa_gut_su_freq.pickle','rb') as f:\n",
    "    total_freq=pickle.load(f)\n",
    "    \n",
    "with open('./preprocessing/vocab_without_josa_gut_su.pickle','rb') as h:\n",
    "    voc=pickle.load(h)\n",
    "    \n",
    "not_unk=list(voc.keys())\n",
    "\n",
    "freq_sum=0\n",
    "\n",
    "def summing(x):\n",
    "    \n",
    "    global freq_sum\n",
    "    freq_sum+=x\n",
    "\n",
    "_=list(map(lambda x:summing(total_freq[x]),not_unk))\n",
    "\n",
    "ratio=(sum(total_freq.values())-freq_sum)/sum(total_freq.values())\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unk 빈도수가 문제는 아닌듯??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
