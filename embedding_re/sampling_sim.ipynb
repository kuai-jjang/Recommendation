{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "class close_words:\n",
    "    def __init__(self,model,w2i,pca=False,n=5):\n",
    "        self.model=model\n",
    "        self.w2i=w2i\n",
    "        self.n=n\n",
    "        self.i2w=dict(zip(w2i.values(),w2i.keys()))\n",
    "        pca = PCA(n_components=2)\n",
    "        self.embeddings=model['embedding_in.weight'] if not pca else torch.tensor(pca.fit_transform(model['embedding_in.weight']))\n",
    "        \n",
    "        \n",
    "    def input_word(self,word):\n",
    "        self.sample_idx=self.w2i[word]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "    \n",
    "    def sample_word(self):\n",
    "        \n",
    "        self.sample_idx=random.sample(list(self.w2i.values()),1)[0]\n",
    "        self.sampled_word=self.i2w[self.sample_idx]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "        print('임의의 단어:',self.sampled_word)\n",
    "    \n",
    "    def l2_dist(self):\n",
    "        \n",
    "        self.trial=torch.mul((self.embeddings-self.sample_vec),(self.embeddings-self.sample_vec))\n",
    "        self.trial[self.sample_idx]=100\n",
    "        self.trial=torch.sum(self.trial,dim=1)\n",
    "        #rec=torch.argmax(-self.trial).item()\n",
    "        idexes=torch.argsort(-self.trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        print(len(self.i2w))\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i): #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "                \n",
    "    def cos_sim(self):\n",
    "      \n",
    "        self.trial=torch.div(torch.sum(torch.mul(self.embeddings,self.sample_vec),dim=1),torch.sum(torch.mul(self.embeddings,self.embeddings),dim=1))\n",
    "     \n",
    "        self.trial[self.sample_idx]=-100\n",
    "        idexes=torch.argsort(self.trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i)+1: #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 사이즈: torch.Size([41035, 256])\n",
      "사전 사이즈: 41034\n"
     ]
    }
   ],
   "source": [
    "#model=torch.load('./w2v_withoud_ns',map_location='cpu')\n",
    "model=torch.load('./training_with_2017_2_epoch_7',map_location='cpu')['state_dict']\n",
    "\n",
    "with open('./preprocessing//vocab_without_josa_gut_su.pickle','rb') as f:\n",
    "    w2i=pickle.load(f)\n",
    "    \n",
    "print('임베딩 사이즈:',model['embedding_in.weight'].shape)  #unk 랑 embedding이 0부터인것을 몰랐음. 0은 버리면 됨\n",
    "print('사전 사이즈:',len(w2i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 단어: ('담소', 'Noun')\n",
      "41034\n",
      "1 번째 가까운 단어: ('친절해요', 'Adjective')\n",
      "2 번째 가까운 단어: ('던지시면', 'Verb')\n",
      "3 번째 가까운 단어: ('맞거나', 'Verb')\n",
      "4 번째 가까운 단어: ('1/4', 'Number')\n",
      "5 번째 가까운 단어: ('하셔야겠지만', 'Verb')\n",
      "임의의 단어 ('담소', 'Noun')\n",
      "1 번째 가까운 단어: ('지엽', 'Noun')\n",
      "2 번째 가까운 단어: ('낭비', 'Noun')\n",
      "3 번째 가까운 단어: ('오메가', 'Noun')\n",
      "4 번째 가까운 단어: ('알아가야', 'Verb')\n",
      "5 번째 가까운 단어: ('YOU', 'Alpha')\n"
     ]
    }
   ],
   "source": [
    "showing=close_words(model,w2i,pca=False)\n",
    "showing.sample_word()\n",
    "showing.l2_dist()\n",
    "print('임의의 단어',showing.sampled_word)\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41034\n",
      "1 번째 가까운 단어: ('때', 'Noun')\n",
      "2 번째 가까운 단어: ('고려사', 'Noun')\n",
      "3 번째 가까운 단어: ('팀플', 'Noun')\n",
      "4 번째 가까운 단어: ('받습니다', 'Verb')\n",
      "5 번째 가까운 단어: ('실제', 'Noun')\n",
      "1 번째 가까운 단어: ('낭비', 'Noun')\n",
      "2 번째 가까운 단어: ('뭡', 'Noun')\n",
      "3 번째 가까운 단어: ('서창', 'Noun')\n",
      "4 번째 가까운 단어: ('오메가', 'Noun')\n",
      "5 번째 가까운 단어: ('모르실', 'Verb')\n"
     ]
    }
   ],
   "source": [
    "showing.input_word(('필기','Noun'))\n",
    "showing.l2_dist()\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNK 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014246669941204219\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessing/vocab_without_josa_gut_su_freq.pickle','rb') as f:\n",
    "    total_freq=pickle.load(f)\n",
    "    \n",
    "with open('./preprocessing/vocab_without_josa_gut_su.pickle','rb') as h:\n",
    "    voc=pickle.load(h)\n",
    "    \n",
    "not_unk=list(voc.keys())\n",
    "\n",
    "freq_sum=0\n",
    "\n",
    "def summing(x):\n",
    "    \n",
    "    global freq_sum\n",
    "    freq_sum+=x\n",
    "\n",
    "_=list(map(lambda x:summing(total_freq[x]),not_unk))\n",
    "\n",
    "ratio=(sum(total_freq.values())-freq_sum)/sum(total_freq.values())\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unk 빈도수가 문제는 아닌듯??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
