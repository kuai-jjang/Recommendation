{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "class close_words:\n",
    "    def __init__(self,model,w2i,pca=False,n=5):\n",
    "        self.model=model\n",
    "        self.w2i=w2i\n",
    "        self.n=n\n",
    "        self.i2w=dict(zip(w2i.values(),w2i.keys()))\n",
    "        pca = PCA(n_components=5)\n",
    "        self.embeddings=model['embedding_in.weight'] if not pca else torch.tensor(pca.fit_transform(model['embedding_in.weight']))\n",
    "        \n",
    "        \n",
    "    def input_word(self,word):\n",
    "        self.sample_idx=self.w2i[word]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "    \n",
    "    def sample_word(self):\n",
    "        \n",
    "        self.sample_idx=random.sample(list(self.w2i.values()),1)[0]\n",
    "        self.sampled_word=self.i2w[self.sample_idx]\n",
    "        self.sample_vec=self.embeddings[self.sample_idx]\n",
    "        print('임의의 단어:',self.sampled_word)\n",
    "    \n",
    "    def l2_dist(self):\n",
    "        \n",
    "        self.trial=torch.mul((self.embeddings-self.sample_vec),(self.embeddings-self.sample_vec))\n",
    "        self.trial[self.sample_idx]=100\n",
    "        self.trial=torch.sum(self.trial,dim=1)\n",
    "        #rec=torch.argmax(-self.trial).item()\n",
    "        idexes=torch.argsort(-self.trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        print(len(self.i2w))\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i): #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "                \n",
    "    def cos_sim(self):\n",
    "      \n",
    "        self.trial=torch.div(torch.sum(torch.mul(self.embeddings,self.sample_vec),dim=1),torch.sum(torch.mul(self.embeddings,self.embeddings),dim=1))\n",
    "     \n",
    "        self.trial[self.sample_idx]=-100\n",
    "        idexes=torch.argsort(self.trial, descending=True)[:self.n]\n",
    "        step=1\n",
    "        for idx in idexes:\n",
    "            if idx==len(w2i)+1: #unk 제거\n",
    "                continue\n",
    "            print(step,'번째 가까운 단어:',self.i2w[idx.item()])\n",
    "            step+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 사이즈: torch.Size([22795, 256])\n",
      "사전 사이즈: 22794\n"
     ]
    }
   ],
   "source": [
    "#model=torch.load('./w2v_withoud_ns',map_location='cpu')\n",
    "model=torch.load('./training_with_mecab_2_epoch_7',map_location='cpu')['state_dict']\n",
    "\n",
    "with open('./preprocessing//vocab_mecab.pickle','rb') as f:\n",
    "    w2i=pickle.load(f)\n",
    "    \n",
    "print('임베딩 사이즈:',model['embedding_in.weight'].shape)  #unk 랑 embedding이 0부터인것을 몰랐음. 0은 버리면 됨\n",
    "print('사전 사이즈:',len(w2i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 단어: ('보관', 'NNG')\n",
      "22794\n",
      "1 번째 가까운 단어: ('일원', 'NNG')\n",
      "2 번째 가까운 단어: ('지양', 'NNG')\n",
      "3 번째 가까운 단어: ('주지', 'NNG')\n",
      "4 번째 가까운 단어: ('시츠', 'NNP')\n",
      "5 번째 가까운 단어: ('냄새', 'NNG')\n",
      "6 번째 가까운 단어: ('시골', 'NNG')\n",
      "7 번째 가까운 단어: ('웠어요강추합니당', 'UNKNOWN')\n",
      "8 번째 가까운 단어: ('뽑히', 'VV')\n",
      "9 번째 가까운 단어: ('승차', 'NNG')\n",
      "10 번째 가까운 단어: ('맞물리', 'VV')\n",
      "임의의 단어 ('보관', 'NNG')\n",
      "1 번째 가까운 단어: ('배연재', 'NNP')\n",
      "2 번째 가까운 단어: ('유성', 'NNG')\n",
      "3 번째 가까운 단어: ('아실', 'VV+EP+ETM')\n",
      "4 번째 가까운 단어: ('프리미어', 'NNP')\n",
      "5 번째 가까운 단어: ('notation', 'SL')\n",
      "6 번째 가까운 단어: ('모범', 'NNG')\n",
      "7 번째 가까운 단어: ('Hawaiian', 'SL')\n",
      "8 번째 가까운 단어: ('서면', 'NNG')\n",
      "9 번째 가까운 단어: ('대할', 'VV+ETM')\n",
      "10 번째 가까운 단어: ('제자', 'NNG')\n"
     ]
    }
   ],
   "source": [
    "showing=close_words(model,w2i,pca=True,n=10)\n",
    "showing.sample_word()\n",
    "showing.l2_dist()\n",
    "print('임의의 단어',showing.sampled_word)\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22794\n",
      "1 번째 가까운 단어: ('학점', 'NNG')\n",
      "2 번째 가까운 단어: ('기말고사', 'NNG')\n",
      "3 번째 가까운 단어: ('입니다', 'VCP+EF')\n",
      "4 번째 가까운 단어: ('받', 'VV')\n",
      "5 번째 가까운 단어: ('같', 'VA')\n",
      "6 번째 가까운 단어: ('번', 'NNBC')\n",
      "7 번째 가까운 단어: ('않', 'VX')\n",
      "8 번째 가까운 단어: ('내용', 'NNG')\n",
      "9 번째 가까운 단어: ('있', 'VX')\n",
      "10 번째 가까운 단어: ('때문', 'NNB')\n",
      "1 번째 가까운 단어: ('화남', 'NNG')\n",
      "2 번째 가까운 단어: ('이상욱', 'NNP')\n",
      "3 번째 가까운 단어: ('써온', 'VV+EC+VX+ETM')\n",
      "4 번째 가까운 단어: ('정순', 'NNG')\n",
      "5 번째 가까운 단어: ('정교수', 'NNG')\n",
      "6 번째 가까운 단어: ('공급', 'NNG')\n",
      "7 번째 가까운 단어: ('못함', 'VX+ETN')\n",
      "8 번째 가까운 단어: ('advance', 'SL')\n",
      "9 번째 가까운 단어: ('20000', 'SN')\n",
      "10 번째 가까운 단어: ('신뢰', 'NNG')\n"
     ]
    }
   ],
   "source": [
    "showing.input_word(('중간', 'NNG'))\n",
    "showing.l2_dist()\n",
    "showing.cos_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNK 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014246669941204219\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessing/vocab_without_josa_gut_su_freq.pickle','rb') as f:\n",
    "    total_freq=pickle.load(f)\n",
    "    \n",
    "with open('./preprocessing/vocab_without_josa_gut_su.pickle','rb') as h:\n",
    "    voc=pickle.load(h)\n",
    "    \n",
    "not_unk=list(voc.keys())\n",
    "\n",
    "freq_sum=0\n",
    "\n",
    "def summing(x):\n",
    "    \n",
    "    global freq_sum\n",
    "    freq_sum+=x\n",
    "\n",
    "_=list(map(lambda x:summing(total_freq[x]),not_unk))\n",
    "\n",
    "ratio=(sum(total_freq.values())-freq_sum)/sum(total_freq.values())\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unk 빈도수가 문제는 아닌듯??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
