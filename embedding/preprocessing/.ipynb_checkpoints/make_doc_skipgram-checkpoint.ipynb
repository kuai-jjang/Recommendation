{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limaries30\\.conda\\envs\\python3.6\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\limaries30\\.conda\\envs\\python3.6\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from compare_pos_tag import sampling_by_length,preprocessing\n",
    "from konlpy.tag import Okt\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "my_data=pd.read_csv(r'C:\\tensor_code\\kluebot\\data\\raw\\2017_1.csv')\n",
    "lecture_sentences=preprocessing(my_data.LectureEval.values)\n",
    "tokenizer=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizing:\n",
    "    \n",
    "    def __init__(self,tokenizer,dataframe,doc_id,window_size=3):\n",
    "        \n",
    "        self.tokenizer=tokenizer\n",
    "        self.dataframe=dataframe\n",
    "        self.doc_id=doc_id\n",
    "        self.doc_set={}\n",
    "        \n",
    "        self.window_size=3\n",
    "\n",
    "    def make_pos(self,x):\n",
    "        for i in x['LectureEval']:\n",
    "            a=self.tokenizer.pos(i)\n",
    "            self.update_(x['doc_name'],a)\n",
    "            \n",
    "    def update_(self,x,a):\n",
    "        \n",
    "        token_length=len(a)\n",
    "        \n",
    "        if len(token_length)>=self.window_size:\n",
    "            skip_set=self.skipgram(a)\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if doc_id[x] not in self.doc_set.keys():\n",
    "             self.doc_set[doc_id[x]]=[]\n",
    "                \n",
    "        self.doc_set[doc_id[x]].append(a)\n",
    "        \n",
    "\n",
    "    \n",
    "    def skipgram(self,x):\n",
    "        skipgram_set=[]\n",
    "        for i in range(len(x)-self.window_size+1):\n",
    "            skipgram_set.append(x[i:i+self.window_size-1])\n",
    "            \n",
    "        return skipgram_set\n",
    "    \n",
    "    def doc_tag(self,x):\n",
    "        pass\n",
    "        \n",
    "def pre_dataframe(dataframe):\n",
    "    \n",
    "    doc_frame=pd.Series(zip(dataframe.ProfessorName.values,dataframe.className.values))\n",
    "    doc_merge=pd.concat([doc_frame.rename('doc_name'),dataframe.LectureEval],axis=1)\n",
    "\n",
    "    return doc_merge\n",
    "            \n",
    "    \n",
    "def make2string(x):\n",
    "\n",
    "    lecutres=ast.literal_eval(x)\n",
    "\n",
    "    return lecutres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=pre_dataframe(my_data)\n",
    "trial['LectureEval']=trial['LectureEval'].map(lambda x:make2string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./doc_id.pickle','rb') as f:\n",
    "    doc_id=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hehe=tokenizing(tokenizer,trial,doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "5       None\n",
       "6       None\n",
       "7       None\n",
       "8       None\n",
       "9       None\n",
       "10      None\n",
       "11      None\n",
       "12      None\n",
       "13      None\n",
       "14      None\n",
       "15      None\n",
       "16      None\n",
       "17      None\n",
       "18      None\n",
       "19      None\n",
       "20      None\n",
       "21      None\n",
       "22      None\n",
       "23      None\n",
       "24      None\n",
       "25      None\n",
       "26      None\n",
       "27      None\n",
       "28      None\n",
       "29      None\n",
       "        ... \n",
       "3530    None\n",
       "3531    None\n",
       "3532    None\n",
       "3533    None\n",
       "3534    None\n",
       "3535    None\n",
       "3536    None\n",
       "3537    None\n",
       "3538    None\n",
       "3539    None\n",
       "3540    None\n",
       "3541    None\n",
       "3542    None\n",
       "3543    None\n",
       "3544    None\n",
       "3545    None\n",
       "3546    None\n",
       "3547    None\n",
       "3548    None\n",
       "3549    None\n",
       "3550    None\n",
       "3551    None\n",
       "3552    None\n",
       "3553    None\n",
       "3554    None\n",
       "3555    None\n",
       "3556    None\n",
       "3557    None\n",
       "3558    None\n",
       "3559    None\n",
       "Length: 3560, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.apply(lambda x:hehe.make_pos(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
